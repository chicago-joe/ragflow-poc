#!/bin/bash

# RAGFlow Complete Setup Script
# This script sets up Ollama + RAGFlow with GPU support and development container

echo "=== RAGFlow Complete Setup (Ollama + GPU + Dev) ==="
echo ""

# Parse command line arguments
FORCE_CPU=false
SKIP_OLLAMA=false

while [[ $# -gt 0 ]]; do
  case $1 in
    --cpu)
      FORCE_CPU=true
      shift
      ;;
    --skip-ollama)
      SKIP_OLLAMA=true
      shift
      ;;
    --help)
      echo "Usage: $0 [OPTIONS]"
      echo ""
      echo "Options:"
      echo "  --cpu           Force CPU-only mode (disable GPU)"
      echo "  --skip-ollama   Skip Ollama setup (assume already running)"
      echo "  --help          Show this help message"
      echo ""
      echo "This script will:"
      echo "  â€¢ Set up Ollama with llama3.2 and bge-m3 models"
      echo "  â€¢ Start RAGFlow with GPU support (if available)"
      echo "  â€¢ Enable development container for PyCharm/Jupyter"
      echo "  â€¢ Configure all API keys and model settings"
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      echo "Use --help for usage information."
      exit 1
      ;;
  esac
done

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Error: Docker is not running. Please start Docker and try again."
    exit 1
fi

echo "âœ… Docker is running"

# Auto-detect GPU availability
USE_GPU=false
if [[ "$FORCE_CPU" != "true" ]]; then
    if command -v nvidia-smi &> /dev/null && nvidia-smi > /dev/null 2>&1; then
        USE_GPU=true
        echo "ğŸ® NVIDIA GPU detected! Enabling GPU acceleration."
        
        # Check for nvidia-container-runtime
        if ! docker info 2>/dev/null | grep -q nvidia; then
            echo "âš ï¸  Warning: nvidia-container-runtime not detected in Docker."
            echo "   GPU support may not work properly."
            echo "   Install nvidia-container-toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
        fi
    else
        echo "â„¹ï¸  No GPU detected or nvidia-smi not available. Using CPU mode."
    fi
else
    echo "ğŸ–¥ï¸  CPU mode forced."
fi

# Set compose file based on GPU availability
if [[ "$USE_GPU" == "true" ]]; then
    COMPOSE_FILE="docker-compose-gpu.yml"
    echo "ğŸ“‹ Using GPU configuration: $COMPOSE_FILE"
else
    COMPOSE_FILE="docker-compose.yml"
    echo "ğŸ“‹ Using CPU configuration: $COMPOSE_FILE"
fi

# Setup Ollama
if [[ "$SKIP_OLLAMA" != "true" ]]; then
    echo ""
    echo "ğŸ¤– Setting up Ollama..."
    
    # Check if Ollama container already exists
    if docker ps -a --format "table {{.Names}}" | grep -q "^ollama$"; then
        echo "â„¹ï¸  Ollama container already exists. Stopping and removing..."
        docker stop ollama > /dev/null 2>&1 || true
        docker rm ollama > /dev/null 2>&1 || true
    fi
    
    # Start Ollama container
    echo "ğŸš€ Starting Ollama container..."
    if [[ "$USE_GPU" == "true" ]]; then
        # Start Ollama with GPU support
        docker run -d --name ollama --gpus all -p 11434:11434 --restart unless-stopped ollama/ollama
    else
        # Start Ollama in CPU mode
        docker run -d --name ollama -p 11434:11434 --restart unless-stopped ollama/ollama
    fi
    
    # Wait for Ollama to start
    echo "â³ Waiting for Ollama to start..."
    sleep 15
    
    # Test Ollama connectivity
    echo "ğŸ” Testing Ollama connectivity..."
    max_attempts=30
    attempt=1
    while [ $attempt -le $max_attempts ]; do
        if curl -s http://localhost:11434/ > /dev/null; then
            echo "âœ… Ollama is accessible at http://localhost:11434/"
            break
        fi
        
        if [ $attempt -eq $max_attempts ]; then
            echo "âŒ Error: Cannot connect to Ollama after $max_attempts attempts."
            echo "Please check if the container is running: docker logs ollama"
            exit 1
        fi
        
        echo "   Attempt $attempt/$max_attempts..."
        sleep 2
        ((attempt++))
    done
    
    # Pull required models
    echo "ğŸ“¦ Pulling Ollama models..."
    echo "   This may take several minutes depending on your internet connection..."
    
    echo "ğŸ“¥ Pulling llama3.2 for chat..."
    if ! docker exec ollama ollama pull llama3.2; then
        echo "âŒ Error: Failed to pull llama3.2 model"
        exit 1
    fi
    
    echo "ğŸ“¥ Pulling bge-m3 for embeddings..."
    if ! docker exec ollama ollama pull bge-m3; then
        echo "âŒ Error: Failed to pull bge-m3 model"
        exit 1
    fi
    
    # Test model availability
    echo "ğŸ§ª Verifying models..."
    docker exec ollama ollama list
    
    # Connect Ollama to RAGFlow network for internal communication
    echo "ğŸ”— Connecting Ollama to RAGFlow network..."
    docker network connect docker_ragflow ollama 2>/dev/null || echo "   (Already connected or network not found)"
    
    echo "âœ… Ollama setup complete!"
else
    echo "â­ï¸  Skipping Ollama setup"
    
    # Still test connectivity if skipping
    if curl -s http://localhost:11434/ > /dev/null; then
        echo "âœ… Ollama is accessible at http://localhost:11434/"
    else
        echo "âš ï¸  Warning: Ollama not accessible. Make sure it's running before starting RAGFlow."
    fi
fi

echo ""
echo "ğŸ—ï¸  Setting up RAGFlow..."

# Set proper permissions for volume directories
echo "ğŸ“ Setting up volume directories..."
mkdir -p ./volumes/{esdata01,osdata01,infinity_data,mysql_data,minio_data,redis_data,ragflow_data,ragflow_uploads}
mkdir -p ./volumes/{dev_workspace,dev_notebooks,dev_data,dev_scripts}
sudo chown -R 1000:1000 ./volumes/
chmod -R 755 ./volumes/

echo "âœ… Volume directories configured"

# Ensure Docker Compose profiles are set correctly
echo "ğŸ”§ Checking environment configuration..."
if grep -q "^COMPOSE_PROFILES=" .env; then
    echo "âœ… COMPOSE_PROFILES found in .env"
else
    echo "âš ï¸  COMPOSE_PROFILES not explicitly set, using default from DOC_ENGINE"
fi

# Display current configuration
echo ""
echo "=== Current Configuration ==="
echo "â€¢ Configuration: $COMPOSE_FILE"
echo "â€¢ GPU Enabled: $USE_GPU"
echo "â€¢ Dev Container: Enabled"
echo "â€¢ Ollama: $(if [[ "$SKIP_OLLAMA" == "true" ]]; then echo "External"; else echo "Integrated"; fi)"
echo "â€¢ UI Port: $(grep SVR_HTTP_PORT .env | cut -d'=' -f2)"
echo "â€¢ MySQL Port: $(grep MYSQL_PORT .env | cut -d'=' -f2)"
echo "â€¢ MinIO Console Port: $(grep MINIO_CONSOLE_PORT .env | cut -d'=' -f2)"
echo "â€¢ Elasticsearch Port: $(grep ES_PORT .env | cut -d'=' -f2)"
echo "â€¢ Redis Port: $(grep REDIS_PORT .env | cut -d'=' -f2)"
echo ""

# Start the services
echo "ğŸš€ Starting RAGFlow services..."
docker-compose -f "$COMPOSE_FILE" down
docker-compose -f "$COMPOSE_FILE" pull
docker-compose -f "$COMPOSE_FILE" up -d

echo ""
echo "â³ Waiting for services to start..."
sleep 45

# Check service status
echo ""
echo "=== Service Status ==="
docker-compose -f "$COMPOSE_FILE" ps

echo ""
echo "ğŸ‰ Complete setup finished!"
echo ""

# Show comprehensive access information
echo "=== Access Information ==="
echo "ğŸŒ RAGFlow UI: http://localhost:8080"
echo "ğŸ“§ Login: admin@ragflow.io"
echo "ğŸ” Password: admin"
echo "âš ï¸  IMPORTANT: Change the admin password after first login!"
echo ""
echo "ğŸ¤– Ollama API: http://localhost:11434"
echo "ğŸ“š RAGFlow API: http://localhost:$(grep SVR_HTTP_PORT .env | cut -d'=' -f2)"
echo "ğŸ“Š MinIO Console: http://localhost:$(grep MINIO_CONSOLE_PORT .env | cut -d'=' -f2)"
echo "ğŸ” Elasticsearch: http://localhost:$(grep ES_PORT .env | cut -d'=' -f2)"
echo ""
echo "ğŸ› ï¸  Development Environment:"
echo "â€¢ Jupyter Lab: http://localhost:8888 (if START_JUPYTER=true in .env)"
echo "â€¢ PyCharm SSH: localhost:2222 (user: root, pass: ragflow123)"
echo ""

echo "=== API Configuration ==="
echo "ğŸ”‘ RAGFlow API Key Setup:"
echo "   1. Login to RAGFlow UI"
echo "   2. Click avatar â†’ API page"
echo "   3. Generate/copy your API key"
echo "   4. Use: Authorization: Bearer YOUR_API_KEY"
echo ""
echo "ğŸ¤– Model Configuration:"
echo "   â€¢ Chat Model: llama3.2"
echo "   â€¢ Embedding Model: bge-m3"
echo "   â€¢ Ollama API Key: [Configure in .env file]"
echo "   â€¢ HF Token: [Configure in .env file]"
echo ""

echo "=== Useful Commands ==="
echo "ğŸ” Check logs:"
echo "   docker-compose -f $COMPOSE_FILE logs -f ragflow"
echo "   docker logs ollama"
echo ""
echo "ğŸ›‘ Stop services:"
echo "   docker-compose -f $COMPOSE_FILE down"
echo "   docker stop ollama"
echo ""
echo "ğŸ”„ Restart services:"
echo "   docker-compose -f $COMPOSE_FILE restart"
echo "   docker restart ollama"
echo ""

if [[ "$USE_GPU" == "true" ]]; then
    echo "ğŸ® GPU acceleration enabled for faster inference!"
    echo ""
fi

echo "ğŸ“ All data persists in: ./volumes/"
echo "ğŸš€ Setup complete! RAGFlow is ready for development and production use."
echo ""

# Final connectivity test
echo "=== Final Connectivity Tests ==="
if curl -s http://localhost:8080 > /dev/null; then
    echo "âœ… RAGFlow UI is accessible"
else
    echo "âš ï¸  RAGFlow UI not yet ready, may need a few more minutes"
fi

if curl -s http://localhost:11434/ > /dev/null; then
    echo "âœ… Ollama API is accessible"
else
    echo "âŒ Ollama API not accessible"
fi

echo ""
echo "Happy coding! ğŸš€"